---
title: "24 Improving Performance"
output: html_document
date: "2023-09-11"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 24 Improving Performance
## 24.1 Introduction

```{r}
library(tidyverse)
library(profvis)
library(bench)
```

## 24.2 Code organisation

```{r}
mean1 <- function(x) mean(x)
mean2 <- function(x) sum(x) / length(x)

x <- runif(1e5)

a <- bench::mark(
  mean1(x),
  mean2(x)
)
a[c("expression", "min", "median", "itr/sec", "n_gc")]
```

## 24.3 Checking for existing solutions

### 24.3.1 Exercises

1. What are faster alternatives to `lm()`? Which are specifically designed to work with larger datasets?

`lm.fit` more barebones and faster
`fastLm` from RcppEigen
`speedlm` from speedglm works on large data sets.


2. What package implements a version of `match()` that’s faster for repeated lookups? How much faster is it?

fastmatch

```{r}
i = rnorm(2e6)
names(i) = as.integer(rnorm(2e6))
```

```{r}
## compare sorting and coalesce
system.time(o <- i[order(names(i))])
system.time(o <- i[fastmatch::coalesce(names(i))])
```

```{r}
## more fair comparison taking the coalesce time (and copy) into account
system.time(tapply(i, names(i), sum))
system.time({ o <- i[fastmatch::coalesce(names(i))]; fastmatch::ctapply(o, names(o), sum) })
```

A lot faster

3. List four functions (not just those in base R) that convert a string into a date time object. What are their strengths and weaknesses?

`as.POSIXct`, `as.Date`, `lubridate::ymd`, `lubridate::mdy`, `ISOdate`. I'm just gonna stick to lubridate


4. Which packages provide the ability to compute a rolling mean?

zoo and RcppRoll

5. What are the alternatives to `optim()`?

optimParallel and optimr

## 24.4 Doing as little as possible
### 24.4.1 `mean()`

```{r}
x <- runif(1e2)

bench::mark(
  mean(x),
  mean.default(x)
)[c("expression", "min", "median", "itr/sec", "n_gc")]
```

```{r}
x <- runif(1e2)
bench::mark(
  mean(x),
  mean.default(x),
  .Internal(mean(x))
)[c("expression", "min", "median", "itr/sec", "n_gc")]
```

```{r}
x <- runif(1e4)
bench::mark(
  mean(x),
  mean.default(x),
  .Internal(mean(x))
)[c("expression", "min", "median", "itr/sec", "n_gc")]
```

### 24.4.2 `as.data.frame()`

```{r}
quickdf <- function(l) {
  class(l) <- "data.frame"
  attr(l, "row.names") <- .set_row_names(length(l[[1]]))
  l
}


l <- lapply(1:26, function(i) runif(1e3))
names(l) <- letters

bench::mark(
  as.data.frame = as.data.frame(l),
  quick_df      = quickdf(l)
)[c("expression", "min", "median", "itr/sec", "n_gc")]
```

```{r}
quickdf(list(x = 1, y = 1:2))
```

### 24.4.3 Exercises

1. What’s the difference between `rowSums()` and `.rowSums()`?

```{r}
rowSums
```

```{r}
.rowSums
```

The later doesn't have flexibility, it expects a specifically formatted matrix while the former while convert for you.

2. Make a faster version of `chisq.test()` that only computes the chi-square test statistic when the input is two numeric vectors with no missing values. You can try simplifying `chisq.test()` or by coding from the mathematical definition.

```{r}
chisq.test(x = c(1,2,3), y = c(11,12,13))

chisq.test.fast <- function(x,y){
  x <- table(x, y)
  n <- sum(x)
  sr <- rowSums(x)
  sc <- colSums(x)
  E <- outer(sr, sc)/n
  sum(sort((x - E)^2/E, decreasing = TRUE))
}

chisq.test.fast(x = c(1,2,3), y = c(11,12,13))

a <- rnorm(1e3)
b <- rnorm(1e3)

bench::mark(
  OG = chisq.test(x = a, y = b),
  Me = chisq.test.fast(x = a, y = b),
  check = FALSE
)
```

3. Can you make a faster version of `table()` for the case of an input of two integer vectors with no missing values? Can you use it to speed up your chi-square test?

```{r}
#Cheating cause lazy
table2 <- function(a, b){
  
  a_s <- sort(unique(a))
  b_s <- sort(unique(b))
  
  a_l <- length(a_s)
  b_l <- length(b_s)
  
  dims <- c(a_l, b_l)
  pr <- a_l * b_l
  dn <- list(a = a_s, b = b_s)
  
  bin <- fastmatch::fmatch(a, a_s) +
    a_l * fastmatch::fmatch(b, b_s) - a_l
  y <- tabulate(bin, pr)
  
  y <- array(y, dim = dims, dimnames = dn)
  class(y) <- "table"
  
  y
}

a <- sample(100, 10000, TRUE)
b <- sample(100, 10000, TRUE)

bench::mark(
  table(a, b),
  table2(a, b)
)

####

chisq.test.fast2 <- function(x,y){
  x <- table2(x, y)
  n <- sum(x)
  sr <- rowSums(x)
  sc <- colSums(x)
  E <- outer(sr, sc)/n
  sum(sort((x - E)^2/E, decreasing = TRUE))
}

a <- rnorm(1e3)
b <- rnorm(1e3)

bench::mark(
  OG = chisq.test(x = a, y = b),
  Me = chisq.test.fast(x = a, y = b),
  Me2 = chisq.test.fast2(x = a, y = b),
  check = FALSE
)
```

## 24.5 Vectorise

```{r}
rowAny <- function(x) rowSums(x) > 0
rowAll <- function(x) rowSums(x) == ncol(x)
```

```{r}
lookup <- setNames(as.list(sample(100, 26)), letters)

x1 <- "j"
x10 <- sample(letters, 10)
x100 <- sample(letters, 100, replace = TRUE)

bench::mark(
  lookup[x1],
  lookup[x10],
  lookup[x100],
  check = FALSE
)[c("expression", "min", "median", "itr/sec", "n_gc")]
```

### 24.5.1 Exercises

1. The density functions, e.g., `dnorm()`, have a common interface. Which arguments are vectorised over? What does `rnorm(10, mean = 10:1)` do?

```{r}
rnorm(10)
rnorm(10, mean = 10:1)

```
`x`, `q`, `p`, `mean`, and `sd` are all vectorized. rnorm will create 10 random numbers from 10 different distributions, the mean from each distribution comes from the value in the vector `[10:1]` hence why the values are going down in the result

2.Compare the speed of `apply(x, 1, sum)` with `rowSums(x)` for varying sizes of x

```{r}
x1 <- matrix(rnorm(1e2), nrow = 10)
x10 <- matrix(rnorm(1e4), nrow = 10)
x100 <- matrix(rnorm(1e6), nrow = 10)
bench::mark(
  apply(x1, 1, sum),
  rowSums(x1),
  apply(x10, 1, sum),
  rowSums(x10),
  apply(x100, 1, sum),
  rowSums(x100),
  check = FALSE
)[c("expression", "min", "median", "itr/sec", "n_gc")]
```
Using `rowSums` is a lot faster.

3. How can you use `crossprod()` to compute a weighted sum? How much faster is it than the naive `sum(x * w)`?

```{r}
x <- rnorm(1e6)
y <- rnorm(1e6, mean = -1)

a <- crossprod(x,y)[[1]]
b <- sum(x*y)
all.equal(a,b)

x1 <- rnorm(1e8)
y1 <- rnorm(1e8, mean = -1)
bench::mark(
  crossprod(x,y)[[1]],
  sum(x*y),
  crossprod(x1,y1)[[1]],
  sum(x1*y1),
  check = FALSE
)[c("expression", "min", "median", "itr/sec", "n_gc")]
```

Slightly faster

## 24.6 Avoiding copies

```{r}
random_string <- function() {
  paste(sample(letters, 50, replace = TRUE), collapse = "")
}
strings10 <- replicate(10, random_string())
strings100 <- replicate(100, random_string())

collapse <- function(xs) {
  out <- ""
  for (x in xs) {
    out <- paste0(out, x)
  }
  out
}

bench::mark(
  loop10  = collapse(strings10),
  loop100 = collapse(strings100),
  vec10   = paste(strings10, collapse = ""),
  vec100  = paste(strings100, collapse = ""),
  check = FALSE
)[c("expression", "min", "median", "itr/sec", "n_gc")]
```

## 24.7 Case study: t-test

```{r}
m <- 1000
n <- 50
X <- matrix(rnorm(m * n, mean = 10, sd = 3), nrow = m)
grp <- rep(1:2, each = n / 2)
```

```{r}
system.time(
  for (i in 1:m) {
    t.test(X[i, ] ~ grp)$statistic
  }
)

system.time(
  for (i in 1:m) {
    t.test(X[i, grp == 1], X[i, grp == 2])$statistic
  }
)
```
```{r}
compT <- function(i){
  t.test(X[i, grp == 1], X[i, grp == 2])$statistic
}
system.time(t1 <- purrr::map_dbl(1:m, compT))
head(t1)
```

```{r}
my_t <- function(x, grp) {
  t_stat <- function(x) {
    m <- mean(x)
    n <- length(x)
    var <- sum((x - m) ^ 2) / (n - 1)

    list(m = m, n = n, var = var)
  }

  g1 <- t_stat(x[grp == 1])
  g2 <- t_stat(x[grp == 2])

  se_total <- sqrt(g1$var / g1$n + g2$var / g2$n)
  (g1$m - g2$m) / se_total
}

system.time(t2 <- purrr::map_dbl(1:m, ~ my_t(X[.,], grp)))

stopifnot(all.equal(t1, t2))
```

```{r}
rowtstat <- function(X, grp){
  t_stat <- function(X) {
    m <- rowMeans(X)
    n <- ncol(X)
    var <- rowSums((X - m) ^ 2) / (n - 1)

    list(m = m, n = n, var = var)
  }

  g1 <- t_stat(X[, grp == 1])
  g2 <- t_stat(X[, grp == 2])

  se_total <- sqrt(g1$var / g1$n + g2$var / g2$n)
  (g1$m - g2$m) / se_total
}
system.time(t3 <- rowtstat(X, grp))

stopifnot(all.equal(t1, t3))
```

## 24.8 Other techniques
